# æ–‡ä»¶åï¼štext_agent.py
from base_agent import BaseAgent
from typing import List
import json
from textwrap import dedent

class QaAgent(BaseAgent):
    """é’ˆå¯¹æ–‡æœ¬æ•°æ®é€æ¡è°ƒç”¨å¤§æ¨¡å‹çš„Agentï¼ˆä¸¤é˜¶æ®µï¼šæ— å…³æ®µæ£€æµ‹ + æƒ…æ„Ÿæ ‡ç­¾é¢„æµ‹ï¼‰"""

    def __init__(self, name: str, model: str, api_key: str, api_base: str):
        super().__init__(name, model, api_key, api_base)

        # å†…éƒ¨çŠ¶æ€å˜é‡
        self._mode = None        # "stage1_init", "stage1_chunk", "stage1_query", "stage2_label"
        self._cache_text = None  # æš‚å­˜å‘é€çš„æ–‡æœ¬
        self._cache_labels = None

    # ---------------------------------------------------------------------
    # æ„é€  prompt çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä¸ BaseAgent.generate é…åˆï¼‰
    # ---------------------------------------------------------------------
    def _build_prompt(self, **kwargs) -> str:
        """æ ¹æ®å½“å‰é˜¶æ®µæ„å»º prompt"""
        if self._mode == "stage2_label":
            text_result = self._cache_text
            print("%%%%",len(text_result))
            label_str = ", ".join(self._cache_labels)
            # return dedent(f"""
            #     ä¸‹é¢æˆ‘å°†å‘é€ç»™ä½ ä¸€æ®µæ–‡æœ¬ä»¥åŠä¸€ä¸ªé—®é¢˜ï¼Œè¯·ä½ åˆ†ææ–‡æœ¬å¹¶ä¸”å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
            #
            #     æ–‡æœ¬å¦‚ä¸‹ï¼š
            #     {text_result}
            #
            #     é—®é¢˜æ˜¯ï¼š
            #     {label_str}
            #
            #     ç®€æ´è¿”å›ç­”æ¡ˆã€‚
            # """).strip()

            return dedent(f"""
            I will provide you a text and a question. Please answer the question **directly and concisely** using only the information in the text. 
            Do **not** start your answer with phrases like "Based on the text" or any other introductory words. 
            Do **not** provide explanations, interpretations, or extra commentary.
            
            For example:
            "problem": "what is descriptive norm", "answer": "Individual perception of others' actual behavior", 
             
            Text:
            {text_result}

            Question:
            {label_str}

            Answer:
            """).strip()


        else:
            # stage1 éƒ¨åˆ†ä½¿ç”¨ messages ç›´æ¥åœ¨ process_texts å†…å¤„ç†
            return ""

    # ---------------------------------------------------------------------
    # å·¥å…·å‡½æ•°ï¼šåˆ†æ®µ
    # ---------------------------------------------------------------------
    def _split_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """å°†æ–‡æœ¬åˆ†æ®µ"""
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    # ---------------------------------------------------------------------
    # ä¸»å‡½æ•°
    # ---------------------------------------------------------------------
    def process_texts(self, texts: List[str], question: List[List[str]], save_path: str = None):
        """
        å¤„ç†æµç¨‹ï¼š
        1ï¸âƒ£ é˜¶æ®µ1ï¼šæ£€æµ‹æ— å…³å†…å®¹ï¼ˆtext_resultï¼‰
        2ï¸âƒ£ é˜¶æ®µ2ï¼šé¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾ï¼ˆresultï¼‰
        """
        results = []

        from openai import OpenAI
        client = OpenAI(api_key=self.api_key, base_url=self.api_base)

        for i, (text, q) in enumerate(zip(texts, question), start=1):
            print(f"\nğŸ”¹ æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(texts)} æ¡æ–‡æœ¬...")
            try:
                # ä¿å­˜ç»“æœ


                # ========== é˜¶æ®µ 1ï¼šæ— å…³å†…å®¹æ£€æµ‹ ==========
                chunks = self._split_text(text)

                # æ„å»º messages
                # messages = [
                #     {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ–‡æœ¬åˆ†æä¸“å®¶ã€‚"},
                #     {"role": "user", "content": dedent(f"""
                #         è¯·ä¸€ç›´è®°ä½æ ¸å¿ƒé—®é¢˜ï¼š{q} ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šåˆ†å‡ æ®µç»™ä½ å‘é€ä¸€æ•´ä»½æ–‡æœ¬ã€‚
                #         è¯·ä½ åˆ†ææ€»ç»“æ–‡æœ¬ä¸­æ‰€æœ‰ä¸é—®é¢˜æœ‰å…³çš„å†…å®¹ï¼Œå¹¶ä¸”ç›¸å…³å†…å®¹å¯èƒ½ä¼šåœ¨åˆ†æ®µå‘é€çš„è¿‡ç¨‹å½“ä¸­è¢«åˆ†å¼€ã€‚
                #         åˆ†æ®µä¹Ÿæ˜¯éšæœºçš„ï¼Œå› æ­¤æ¯ä¸€æ®µçš„ç¬¬ä¸€å¥ä¸æœ€åä¸€å¥å¾ˆæœ‰å¯èƒ½ä¸æ˜¯å®Œæ•´çš„ã€‚
                #         ä½ éœ€è¦ç»“åˆæ•´ä¸ªæ–‡æœ¬å»æ•´ç†æ‰€æœ‰ä¸é—®é¢˜æœ‰å…³çš„å†…å®¹ï¼Œå¹¶å°†è¿™éƒ¨åˆ†å†…å®¹è¿”å›ç»™æˆ‘ã€‚ä¸è¦ç»™æˆ‘ä»»ä½•çš„åˆ†æä¸è§£é‡Šï¼Œåªç»™æˆ‘ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹å³å¯
                #     """).strip()},
                # ]
                messages = [
                    {"role": "system", "content": "You are a text analysis expert."},
                    {"role": "user", "content": dedent(f"""
                        Please always keep the core question in mind: {q}. I will send you a long text in several parts. 
                        Please analyze and summarize all content related to the question. Note that the relevant content may be split across different parts.
                        The segments are sent randomly, so the first and last sentence of each part may not be complete.
                        You need to consider the entire text to organize all content related to the question and return only this content to me. 
                        Do not provide any analysis or explanationâ€”just return the content relevant to the question.
                    """).strip()},
                ]

                # æ·»åŠ æ‰€æœ‰åˆ†æ®µå†…å®¹
                for chunk in chunks:
                    messages.append({"role": "user", "content": chunk})

                # æœ€åè¿½åŠ æé—®
                # messages.append({"role": "user", "content": "ç°åœ¨è¯·æ±‡æ€»å¹¶è¾“å‡ºæ‰€æœ‰ä¸é—®é¢˜æœ‰å…³çš„å¥å­"})
                messages.append({"role": "user",
                                 "content": "Now please summarize and output all sentences related to the question."})

                # è°ƒç”¨æ¨¡å‹
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=messages
                )
                text_result = response.choices[0].message.content.strip()
                print(f"ç¬¬ {i}/{len(texts)} æ¡ä¸é—®é¢˜ç›¸å…³å†…å®¹ä¸ºï¼š{text_result}")

                # ========== é˜¶æ®µ 2ï¼šæƒ…æ„Ÿæ ‡ç­¾é¢„æµ‹ ==========
                print(f"ğŸ“¨ å¼€å§‹å›ç­”é—®é¢˜{q}...")
                self._mode = "stage2_label"
                self._cache_text = text_result
                self._cache_labels = q
                self._build_prompt()
                result = self.generate()
                print(f"âœ… è·å¾—ç­”æ¡ˆï¼š{result}")

                # ä¿å­˜ç»“æœåˆ°åˆ—è¡¨
                # results.append({
                #     "irrelevant_part": text_result,
                #     "predicted_emotion": result
                # })

                result_item = {
                    "irrelevant_part": text_result,
                    "predicted_emotion": result
                }
                results.append(result_item)

                # âœ… ç«‹å³å†™å…¥æ–‡ä»¶ï¼ˆè¿½åŠ ï¼‰
                if save_path:
                    with open(save_path, "a", encoding="utf-8") as f:
                        f.write(f"ã€ç¬¬{i}ä¸ªé—®é¢˜ï¼š{q}ã€‘\n")
                        f.write(f"é—®é¢˜ç›¸å…³æ–‡æœ¬ï¼š{result_item['irrelevant_part']}\n")
                        f.write(f"å›ç­”{i}ç­”æ¡ˆï¼š{result_item['predicted_emotion']}\n")
                        f.write("-" * 50 + "\n")
                        f.flush()

            except Exception as e:
                print(f"[Error] ç¬¬ {i} æ¡æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
                results.append({
                    "irrelevant_part": None,
                    "predicted_emotion": f"[Error] {e}"
                })

        # # ========== ä¿å­˜ç»“æœä¸º log æ–‡ä»¶ ==========
        # if save_path:
        #     with open(save_path, "w", encoding="utf-8") as f:
        #         for idx, r in enumerate(results, start=1):
        #             f.write(f"ã€ç¬¬{idx}æ¡ç»“æœã€‘\n")
        #             f.write(f"é—®é¢˜ç›¸å…³æ–‡æœ¬ï¼š{r['relevant_part']}\n")
        #             f.write(f"å›ç­”ç­”æ¡ˆï¼š{r['predicted_answer']}\n")
        #             f.write("-" * 50 + "\n")

        return results
